<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>LLMSYS</title>
  
  
  <link href="https://llm-sys.github.io/atom.xml" rel="self"/>
  
  <link href="https://llm-sys.github.io/"/>
  <updated>2023-12-09T19:02:08.280Z</updated>
  <id>https://llm-sys.github.io/</id>
  
  <author>
    <name>Fang Taosong</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>Optimization for Modern LLM Online Serving: Continuous Batching and Paged Attention</title>
    <link href="https://llm-sys.github.io/2023/12/10/Optimization-for-Modern-LLM-Online-Serving-Continuous-Batching-and-Paged-Attention/"/>
    <id>https://llm-sys.github.io/2023/12/10/Optimization-for-Modern-LLM-Online-Serving-Continuous-Batching-and-Paged-Attention/</id>
    <published>2023-12-10T01:52:59.000Z</published>
    <updated>2023-12-09T19:02:08.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="Optimization-for-Modern-LLM-Online-Serving-Continuous-Batching-and-Paged-Attention"><a href="#Optimization-for-Modern-LLM-Online-Serving-Continuous-Batching-and-Paged-Attention" class="headerlink" title="Optimization for Modern LLM Online Serving: Continuous Batching and Paged Attention"></a>Optimization for Modern LLM Online Serving: Continuous Batching and Paged Attention</h1><p>‰∏çÁü•‰∏çËßâüïä‰∫ÜÊúâÁÇπ‰πÖÔºåÊú¨Ê¨°Êàë‰ª¨ÁõòÁÇπ‰∏Ä‰∏ãÈù¢ÂêëÂ§ßÊ®°ÂûãÁöÑÊúçÂä°Á≥ªÁªüÔºàÂºïÊìéÔºâÁöÑËÆæËÆ°Áõ∏ÊØî‰∫é‰∏Ä‰∫õ‰º†ÁªüÁöÑÊú∫Âô®Â≠¶‰π†ÊúçÂä°Á≥ªÁªüÁöÑÊîπËøõÔºö</p><h3 id="Continuous-Batching"><a href="#Continuous-Batching" class="headerlink" title="Continuous Batching"></a>Continuous Batching</h3><p>The compute utilization in serving LLMs can be improved by batching multiple requests. Because the requests share the same model weights, the overhead of moving weights is amortized across the requests in a batch, and can be overwhelmed by the computational overhead when the batch size is sufficiently large. However, batching the requests to an LLM service is non-trivial for two reasons. First, the requests may arrive at different times. A naive batching strategy would either make earlier requests wait for later ones or delay the incoming requests until earlier ones finish, leading to significant queueing delays. Second, the requests may have vastly different input and output lengths. A straightforward batching technique would pad the inputs and outputs of the requests to equalize their lengths, wasting GPU computation and memory. </p><p>To address this problem, fine-grained batching mechanisms, such as cellular batching and iteration-level scheduling , have been proposed. Unlike traditional methods that work at the request level, these techniques operate at the iteration level. After each iteration, completed requests are removed from the batch, and new ones are added. Therefore, a new request can be processed after waiting for a single iteration, not waiting for the entire batch to complete. Moreover, with special GPU kernels, these techniques eliminate the need to pad the inputs and outputs. By reducing the queueing delay and the inefficiencies from padding, the fine-grained batching mechanisms significantly increase the throughput of LLM serving.</p><p>Continuous batching is a technique used to optimize the performance of large language models (LLMs) by improving the efficiency of their inference. It is a system-level optimization that can produce 10 times or more difference in actual workloads. The technique is also known as dynamic batching or batching with iteration-level scheduling.</p><p><img src="http://myimg2.constfrost.com//hw/image-20231210015720268.png" alt="image-20231210015720268"></p><p>In traditional static batching, the batch size remains constant throughout the inference process. However, LLMs have an iterative inference process, which means that the length of the generated sequence is not known beforehand. This leads to underutilization of the GPU‚Äôs processing power and memory bandwidth. Continuous batching solves this problem by dynamically adjusting the batch size based on the length of the generated sequence. This allows the GPU to process more sequences in parallel, leading to higher throughput and lower latency.</p><p>Continuous batching is implemented using a technique called iteration-level scheduling. In this technique, the GPU processes multiple sequences in parallel, but each sequence is processed iteratively. The batch size is adjusted dynamically based on the length of the generated sequence. This allows the GPU to process more sequences in parallel, leading to higher throughput and lower latency.</p><p>The following LaTeX formula shows how the length of the generated sequence is used to adjust the batch size:<br>$$<br>batch_size &#x3D; \frac{max_batch_size}{1 + \frac{length}{iteration_size}}<br>$$<br>where <code>max_batch_size</code> is the maximum batch size, length is the length of the generated sequence, and iteration_size is the number of iterations required to generate the sequence.</p><p>Continuous batching is a powerful optimization technique that can significantly improve the performance of LLMs. It is widely used in modern LLM inference systems and has become an essential tool for ML engineers working with large language models.</p><h3 id="Paged-Attention"><a href="#Paged-Attention" class="headerlink" title="Paged Attention"></a>Paged Attention</h3><p><strong>Paged Attention</strong> is an attention algorithm that is inspired by the classical virtual memory and paging techniques in operating systems. It is used to manage memory efficiently in large language models (LLMs) serving systems.</p><p><img src="http://myimg2.constfrost.com//hw/image-20231210011405540.png" alt="image-20231210011405540"></p><p>The key-value cache (KV cache) memory for each request in LLMs serving systems is huge and grows and shrinks dynamically. When managed inefficiently, this memory can be significantly wasted by fragmentation and redundant duplication, limiting the batch size. Paged Attention addresses this problem by allowing non-contiguous storage of contiguous KV tensors. It divides the KV cache of each sequence into blocks, each containing a fixed number of tokens. During attention calculation, it can efficiently locate and retrieve those blocks.</p><p>On top of Paged Attention, vLLM is built, which is an LLM serving system that achieves near-zero waste in KV cache memory and flexible sharing of KV cache within and across requests to further reduce memory usage. <a href="https://arxiv.org/abs/2309.06180">vLLM improves the throughput of popular LLMs by 2-4 √ó with the same level of latency compared to the state-of-the-art systems, such as FasterTransformer and Orca </a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;Optimization-for-Modern-LLM-Online-Serving-Continuous-Batching-and-Paged-Attention&quot;&gt;&lt;a href=&quot;#Optimization-for-Modern-LLM-Online-Ser</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>About</title>
    <link href="https://llm-sys.github.io/2023/12/09/hello-world/"/>
    <id>https://llm-sys.github.io/2023/12/09/hello-world/</id>
    <published>2023-12-09T19:02:08.280Z</published>
    <updated>2023-12-09T19:02:08.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="About"><a href="#About" class="headerlink" title="About"></a>About</h1><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><blockquote><p>This project is inspired by <a href="https://lmsys.org/">LMSYS</a>.</p></blockquote><h2 id="About-LLMSYS"><a href="#About-LLMSYS" class="headerlink" title="About LLMSYS"></a>About LLMSYS</h2><p>The development of large-scale pre-training models has been booming, with new large models emerging every two to three days.</p><p>The main objectives of this project are:</p><ul><li><p>Summarize and address the engineering challenges and training techniques encountered during the training process of large-scale pre-training models.</p></li><li><p>Open-source and maintain a set of high-quality, reusable software tools related to LLM training and inference.</p></li></ul><p>Share the state-of-the-art open-source LLMs.</p><h2 id="About-Maintainer"><a href="#About-Maintainer" class="headerlink" title="About Maintainer"></a>About Maintainer</h2><p>The current maintainer is Fang Taosong, an incoming computer science master‚Äôs student from the University of Chinese Academy of Sciences. His research focuses on LLM training, inference acceleration, and‚Ä¶</p><p>Contact email: <a href="mailto:&#x66;&#97;&#x6e;&#x67;&#116;&#97;&#x6f;&#x73;&#111;&#110;&#103;&#x32;&#48;&#x32;&#x32;&#64;&#105;&#x73;&#99;&#x61;&#x73;&#x2e;&#x61;&#99;&#x2e;&#x63;&#110;">&#x66;&#97;&#x6e;&#x67;&#116;&#97;&#x6f;&#x73;&#111;&#110;&#103;&#x32;&#48;&#x32;&#x32;&#64;&#105;&#x73;&#99;&#x61;&#x73;&#x2e;&#x61;&#99;&#x2e;&#x63;&#110;</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;About&quot;&gt;&lt;a href=&quot;#About&quot; class=&quot;headerlink&quot; title=&quot;About&quot;&gt;&lt;/a&gt;About&lt;/h1&gt;&lt;h2 id=&quot;Introduction&quot;&gt;&lt;a href=&quot;#Introduction&quot; class=&quot;headerli</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Projects</title>
    <link href="https://llm-sys.github.io/2023/12/09/work/"/>
    <id>https://llm-sys.github.io/2023/12/09/work/</id>
    <published>2023-12-09T19:02:08.280Z</published>
    <updated>2023-12-09T19:02:08.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="WORK"><a href="#WORK" class="headerlink" title="WORK"></a>WORK</h1><p>Here are our works:</p><h2 id="med-language-model"><a href="#med-language-model" class="headerlink" title="med-language-model"></a>med-language-model</h2><p>The medical model I trained during my undergraduate graduation project used instruction fine-tuning.</p><ul><li>githubÔºö</li><li>model playgroundÔºö<a href="./medical-model.html">link</a></li></ul><h2 id="open-instruction"><a href="#open-instruction" class="headerlink" title="open-instruction"></a>open-instruction</h2><p>An instruction data annotation tool designed for the era of instruction fine-tuning, including functions such as instructions annotation, demonstration annotation, and open domain annotation.</p><ul><li>gitlab (Still under development, the Chinese information processing laboratory is open source on the intranet. Please contact my email to obtain it.) : <a href="http://git.cipsup.cn/arknet/open-instruction/tree/master/open_instruct">link</a></li></ul>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;WORK&quot;&gt;&lt;a href=&quot;#WORK&quot; class=&quot;headerlink&quot; title=&quot;WORK&quot;&gt;&lt;/a&gt;WORK&lt;/h1&gt;&lt;p&gt;Here are our works:&lt;/p&gt;
&lt;h2 id=&quot;med-language-model&quot;&gt;&lt;a href=&quot;#</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>How to Make a Big Language Model an Agent</title>
    <link href="https://llm-sys.github.io/2023/11/04/How-to-Make-a-Big-Language-Model-an-Agent/"/>
    <id>https://llm-sys.github.io/2023/11/04/How-to-Make-a-Big-Language-Model-an-Agent/</id>
    <published>2023-11-04T21:01:23.000Z</published>
    <updated>2023-12-09T19:02:08.280Z</updated>
    
    <content type="html"><![CDATA[<h1 id="How-to-Make-a-Big-Language-Model-an-Agent"><a href="#How-to-Make-a-Big-Language-Model-an-Agent" class="headerlink" title="How to Make a Big Language Model an Agent"></a>How to Make a Big Language Model an Agent</h1><p>ÈïøÊúü‰ª•Êù•ÔºåÁ†îÁ©∂ËÄÖ‰ª¨‰∏ÄÁõ¥Âú®ËøΩÊ±Ç‰∏é‰∫∫Á±ªÁõ∏ÂΩì„ÄÅ‰πÉËá≥Ë∂ÖË∂ä‰∫∫Á±ªÊ∞¥Âπ≥ÁöÑÈÄöÁî®‰∫∫Â∑•Êô∫ËÉΩÔºàArtificial General IntelligenceÔºåAGIÔºâ„ÄÇÊó©Âú® 1950 Âπ¥‰ª£ÔºåAlan Turing Â∞±Â∞Ü„ÄåÊô∫ËÉΩ„ÄçÁöÑÊ¶ÇÂøµÊâ©Â±ïÂà∞‰∫Ü‰∫∫Â∑•ÂÆû‰ΩìÔºåÂπ∂ÊèêÂá∫‰∫ÜËëóÂêçÁöÑÂõæÁÅµÊµãËØï„ÄÇËøô‰∫õ‰∫∫Â∑•Êô∫ËÉΩÂÆû‰ΩìÈÄöÂ∏∏Ë¢´Áß∞‰∏∫ ‚Äî‚Äî ‰ª£ÁêÜÔºàAgent*Ôºâ„ÄÇ„Äå‰ª£ÁêÜ„ÄçËøô‰∏ÄÊ¶ÇÂøµËµ∑Ê∫ê‰∫éÂì≤Â≠¶ÔºåÊèèËø∞‰∫Ü‰∏ÄÁßçÊã•ÊúâÊ¨≤Êúõ„ÄÅ‰ø°Âøµ„ÄÅÊÑèÂõæ‰ª•ÂèäÈááÂèñË°åÂä®ËÉΩÂäõÁöÑÂÆû‰Ωì„ÄÇÂú®‰∫∫Â∑•Êô∫ËÉΩÈ¢ÜÂüüÔºåËøô‰∏ÄÊúØËØ≠Ë¢´Ëµã‰∫à‰∫Ü‰∏ÄÂ±ÇÊñ∞ÁöÑÂê´‰πâÔºö<strong>ÂÖ∑ÊúâËá™‰∏ªÊÄß„ÄÅÂèçÂ∫îÊÄß„ÄÅÁßØÊûÅÊÄßÂíåÁ§æ‰∫§ËÉΩÂäõÁâπÂæÅÁöÑÊô∫ËÉΩÂÆû‰Ωì</strong>„ÄÇ</p><p>*<em>Agent ÊúØËØ≠ÁöÑ‰∏≠ÊñáËØëÂêçÂπ∂Êú™ÂΩ¢ÊàêÂÖ±ËØÜÔºåÊúâÂ≠¶ËÄÖÂ∞ÜÂÖ∂ÁøªËØë‰∏∫Êô∫ËÉΩ‰Ωì„ÄÅË°å‰∏∫‰Ωì„ÄÅ‰ª£ÁêÜÊàñÊô∫ËÉΩ‰ª£ÁêÜÔºåÊú¨Êñá‰∏≠Âá∫Áé∞ÁöÑ„Äå‰ª£ÁêÜ„ÄçÂíå„ÄåÊô∫ËÉΩ‰ª£ÁêÜ„ÄçÂùáÊåá‰ª£ Agent„ÄÇ</em></p><p>‰ªéÈÇ£Êó∂Ëµ∑Ôºå‰ª£ÁêÜÁöÑËÆæËÆ°Â∞±Êàê‰∏∫‰∫∫Â∑•Êô∫ËÉΩÁ§æÂå∫ÁöÑÁÑ¶ÁÇπ„ÄÇÁÑ∂ËÄåÔºåËøáÂéªÁöÑÂ∑•‰Ωú‰∏ªË¶ÅÈõÜ‰∏≠Âú®Â¢ûÂº∫‰ª£ÁêÜÁöÑÁâπÂÆöËÉΩÂäõÔºåÂ¶ÇÁ¨¶Âè∑Êé®ÁêÜÊàñÂØπÁâπÂÆö‰ªªÂä°ÁöÑÊéåÊè°ÔºàÂõΩÈôÖË±°Ê£ã„ÄÅÂõ¥Ê£ãÁ≠âÔºâ„ÄÇËøô‰∫õÁ†îÁ©∂Êõ¥Âä†Ê≥®ÈáçÁÆóÊ≥ïËÆæËÆ°ÂíåËÆ≠ÁªÉÁ≠ñÁï•ÔºåËÄåÂøΩËßÜ‰∫ÜÊ®°ÂûãÂõ∫ÊúâÁöÑÈÄöÁî®ËÉΩÂäõÁöÑÂèëÂ±ïÔºåÂ¶ÇÁü•ËØÜËÆ∞ÂøÜ„ÄÅÈïøÊúüËßÑÂàí„ÄÅÊúâÊïàÊ≥õÂåñÂíåÈ´òÊïà‰∫íÂä®Á≠â„ÄÇ‰∫ãÂÆûËØÅÊòéÔºå<strong>Â¢ûÂº∫Ê®°ÂûãÂõ∫ÊúâËÉΩÂäõÊòØÊé®Âä®Êô∫ËÉΩ‰ª£ÁêÜËøõ‰∏ÄÊ≠•ÂèëÂ±ïÁöÑÂÖ≥ÈîÆÂõ†Á¥†„ÄÇ</strong></p><p>Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÁöÑÂá∫Áé∞‰∏∫Êô∫ËÉΩ‰ª£ÁêÜÁöÑËøõ‰∏ÄÊ≠•ÂèëÂ±ïÂ∏¶Êù•‰∫ÜÂ∏åÊúõ„ÄÇÂ¶ÇÊûúÂ∞Ü NLP Âà∞ AGI ÁöÑÂèëÂ±ïË∑ØÁ∫øÂàÜ‰∏∫‰∫îÁ∫ßÔºöËØ≠ÊñôÂ∫ì„ÄÅ‰∫íËÅîÁΩë„ÄÅÊÑüÁü•„ÄÅÂÖ∑Ë∫´ÂíåÁ§æ‰ºöÂ±ûÊÄßÔºåÈÇ£‰πàÁõÆÂâçÁöÑÂ§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÂ∑≤ÁªèÊù•Âà∞‰∫ÜÁ¨¨‰∫åÁ∫ßÔºåÂÖ∑Êúâ‰∫íËÅîÁΩëËßÑÊ®°ÁöÑÊñáÊú¨ËæìÂÖ•ÂíåËæìÂá∫„ÄÇÂú®Ëøô‰∏™Âü∫Á°Ä‰∏äÔºåÂ¶ÇÊûúËµã‰∫à LLM-based Agents ÊÑüÁü•Á©∫Èó¥ÂíåË°åÂä®Á©∫Èó¥ÔºåÂÆÉ‰ª¨Â∞ÜËææÂà∞Á¨¨‰∏â„ÄÅÁ¨¨ÂõõÁ∫ß„ÄÇËøõ‰∏ÄÊ≠•Âú∞ÔºåÂ§ö‰∏™‰ª£ÁêÜÈÄöËøá‰∫íÂä®„ÄÅÂêà‰ΩúËß£ÂÜ≥Êõ¥Â§çÊùÇÁöÑ‰ªªÂä°ÔºåÊàñËÄÖÂèçÊò†Âá∫Áé∞ÂÆû‰∏ñÁïåÁöÑÁ§æ‰ºöË°å‰∏∫ÔºåÂàôÊúâÊΩúÂäõÊù•Âà∞Á¨¨‰∫îÁ∫ß ‚Äî‚Äî ‰ª£ÁêÜÁ§æ‰ºö„ÄÇ</p><p><img src="https://pic4.zhimg.com/80/v2-27b3f8b096139aec85ff1eb81aa80c6f_720w.webp" alt="img"></p><h3 id="‰∏Ä‰∏™-Agent-ÁöÑËØûÁîü-‚Äî‚Äî-Agent-Ê°ÜÊû∂"><a href="#‰∏Ä‰∏™-Agent-ÁöÑËØûÁîü-‚Äî‚Äî-Agent-Ê°ÜÊû∂" class="headerlink" title="‰∏Ä‰∏™ Agent ÁöÑËØûÁîü ‚Äî‚Äî Agent Ê°ÜÊû∂"></a><strong>‰∏Ä‰∏™ Agent ÁöÑËØûÁîü ‚Äî‚Äî Agent Ê°ÜÊû∂</strong></h3><p>Êã•ÊúâÂ§ßÊ®°ÂûãÂä†ÊåÅÁöÑÊô∫ËÉΩ‰ª£ÁêÜ‰ºöÊòØ‰ªÄ‰πàÊ†∑Ôºü‰ΩúËÄÖ‰ª¨ÂèóÂà∞ËææÂ∞îÊñá„ÄåÈÄÇËÄÖÁîüÂ≠ò„ÄçÊ≥ïÂàôÁöÑÂêØÂèëÔºåÊèêÂá∫‰∫ÜÂü∫‰∫éÂ§ßÊ®°ÂûãÁöÑÊô∫ËÉΩ‰ª£ÁêÜÈÄöÁî®Ê°ÜÊû∂„ÄÇ‰∏Ä‰∏™‰∫∫Â¶ÇÊûúÊÉ≥Ë¶ÅÂú®Á§æ‰ºö‰∏≠ÁîüÂ≠òÔºåÂ∞±ÂøÖÈ°ªÂ≠¶‰ºöÈÄÇÂ∫îÁéØÂ¢ÉÔºåÂõ†Ê≠§ÈúÄË¶ÅÂÖ∑ÊúâËÆ§Áü•ËÉΩÂäõÔºåÂπ∂‰∏îËÉΩÂ§üÊÑüÁü•„ÄÅÂ∫îÂØπÂ§ñÁïåÁöÑÂèòÂåñ„ÄÇÂêåÊ†∑ÔºåÊô∫ËÉΩ‰ª£ÁêÜÁöÑÊ°ÜÊû∂‰πüÁî±‰∏â‰∏™ÈÉ®ÂàÜÁªÑÊàêÔºö<strong>ÊéßÂà∂Á´ØÔºàBrainÔºâ„ÄÅÊÑüÁü•Á´ØÔºàPerceptionÔºâÂíåË°åÂä®Á´ØÔºàActionÔºâ„ÄÇ</strong></p><ul><li><strong>ÊéßÂà∂Á´Ø</strong>ÔºöÈÄöÂ∏∏Áî± LLMs ÊûÑÊàêÔºåÊòØÊô∫ËÉΩ‰ª£ÁêÜÁöÑÊ†∏ÂøÉ„ÄÇÂÆÉ‰∏ç‰ªÖÂèØ‰ª•Â≠òÂÇ®ËÆ∞ÂøÜÂíåÁü•ËØÜÔºåËøòÊâøÊãÖÁùÄ‰ø°ÊÅØÂ§ÑÁêÜ„ÄÅÂÜ≥Á≠ñÁ≠â‰∏çÂèØÊàñÁº∫ÁöÑÂäüËÉΩ„ÄÇÂÆÉÂèØ‰ª•ÂëàÁé∞Êé®ÁêÜÂíåËÆ°ÂàíÁöÑËøáÁ®ãÔºåÂπ∂ÂæàÂ•ΩÂú∞Â∫îÂØπÊú™Áü•‰ªªÂä°ÔºåÂèçÊò†Âá∫Êô∫ËÉΩ‰ª£ÁêÜÁöÑÊ≥õÂåñÊÄßÂíåËøÅÁßªÊÄß„ÄÇ</li><li><strong>ÊÑüÁü•Á´Ø</strong>ÔºöÂ∞ÜÊô∫ËÉΩ‰ª£ÁêÜÁöÑÊÑüÁü•Á©∫Èó¥‰ªéÁ∫ØÊñáÊú¨ÊãìÂ±ïÂà∞ÂåÖÊã¨ÊñáÊú¨„ÄÅËßÜËßâÂíåÂê¨ËßâÁ≠âÂ§öÊ®°ÊÄÅÈ¢ÜÂüüÔºå‰Ωø‰ª£ÁêÜËÉΩÂ§üÊõ¥ÊúâÊïàÂú∞‰ªéÂë®Âõ¥ÁéØÂ¢É‰∏≠Ëé∑Âèñ‰∏éÂà©Áî®‰ø°ÊÅØ„ÄÇ</li><li><strong>Ë°åÂä®Á´Ø</strong>ÔºöÈô§‰∫ÜÂ∏∏ËßÑÁöÑÊñáÊú¨ËæìÂá∫ÔºåËøòËµã‰∫à‰ª£ÁêÜÂÖ∑Ë∫´ËÉΩÂäõ„ÄÅ‰ΩøÁî®Â∑•ÂÖ∑ÁöÑËÉΩÂäõÔºå‰ΩøÂÖ∂ËÉΩÂ§üÊõ¥Â•ΩÂú∞ÈÄÇÂ∫îÁéØÂ¢ÉÂèòÂåñÔºåÈÄöËøáÂèçÈ¶à‰∏éÁéØÂ¢É‰∫§‰∫íÔºåÁîöËá≥ËÉΩÂ§üÂ°ëÈÄ†ÁéØÂ¢É„ÄÇ</li></ul><p><img src="https://pic2.zhimg.com/80/v2-8b47063c5a1d6b64bdc7c6713dbc8951_720w.webp" alt="img"></p><p><em>LLM-based Agent ÁöÑÊ¶ÇÂøµÊ°ÜÊû∂ÔºåÂåÖÂê´‰∏â‰∏™ÁªÑÊàêÈÉ®ÂàÜÔºöÊéßÂà∂Á´ØÔºàBrainÔºâ„ÄÅÊÑüÁü•Á´ØÔºàPerceptionÔºâÂíåË°åÂä®Á´ØÔºàActionÔºâ</em>„ÄÇ</p><p>‰ΩúËÄÖ‰ª¨Áî®‰∏Ä‰∏™‰æãÂ≠êÊù•ËØ¥ÊòéÊù•‰∫Ü LLM-based Agent ÁöÑÂ∑•‰ΩúÊµÅÁ®ãÔºöÂΩì‰∫∫Á±ªËØ¢ÈóÆÊòØÂê¶‰ºö‰∏ãÈõ®Êó∂ÔºåÊÑüÁü•Á´ØÔºàPerceptionÔºâÂ∞ÜÊåá‰ª§ËΩ¨Êç¢‰∏∫ LLMs ÂèØ‰ª•ÁêÜËß£ÁöÑË°®Á§∫„ÄÇÁÑ∂ÂêéÊéßÂà∂Á´ØÔºàBrainÔºâÂºÄÂßãÊ†πÊçÆÂΩìÂâçÂ§©Ê∞îÂíå‰∫íËÅîÁΩë‰∏äÁöÑÂ§©Ê∞îÈ¢ÑÊä•ËøõË°åÊé®ÁêÜÂíåË°åÂä®ËßÑÂàí„ÄÇÊúÄÂêéÔºåË°åÂä®Á´ØÔºàActionÔºâÂÅöÂá∫ÂìçÂ∫îÂπ∂Â∞ÜÈõ®‰ºûÈÄíÁªô‰∫∫Á±ª„ÄÇ</p><p>ÈÄöËøáÈáçÂ§ç‰∏äËø∞ËøáÁ®ãÔºåÊô∫ËÉΩ‰ª£ÁêÜÂèØ‰ª•‰∏çÊñ≠Ëé∑ÂæóÂèçÈ¶àÂπ∂‰∏éÁéØÂ¢É‰∫§‰∫í„ÄÇ</p><h4 id="ÊéßÂà∂Á´ØÔºöBrain"><a href="#ÊéßÂà∂Á´ØÔºöBrain" class="headerlink" title="ÊéßÂà∂Á´ØÔºöBrain"></a><strong>ÊéßÂà∂Á´ØÔºöBrain</strong></h4><p>ÊéßÂà∂Á´Ø‰Ωú‰∏∫Êô∫ËÉΩ‰ª£ÁêÜÊúÄÊ†∏ÂøÉÁöÑÁªÑÊàêÊàêÂàÜÔºå‰ΩúËÄÖ‰ª¨‰ªé‰∫î‰∏™ÊñπÈù¢Â±ïÂºÄ‰ªãÁªçÂÖ∂ËÉΩÂäõÔºö</p><p><strong>Ëá™ÁÑ∂ËØ≠Ë®Ä‰∫§‰∫íÔºö</strong>ËØ≠Ë®ÄÊòØÊ≤üÈÄöÁöÑÂ™í‰ªãÔºåÂÖ∂‰∏≠ÂåÖÂê´ÁùÄ‰∏∞ÂØåÁöÑ‰ø°ÊÅØ„ÄÇÂæóÁõä‰∫é LLMs Âº∫Â§ßÁöÑËá™ÁÑ∂ËØ≠Ë®ÄÁîüÊàêÂíåÁêÜËß£ËÉΩÂäõÔºåÊô∫ËÉΩ‰ª£ÁêÜËÉΩÂ§üÈÄöËøáËá™ÁÑ∂ËØ≠Ë®Ä‰∏éÂ§ñÁïåËøõË°åÂ§öËΩÆ‰∫§‰∫íÔºåËøõËÄåÂÆûÁé∞ÁõÆÊ†á„ÄÇÂÖ∑‰ΩìËÄåË®ÄÔºåÂèØ‰ª•ÂàÜ‰∏∫‰∏§‰∏™ÊñπÈù¢Ôºö</p><ul><li>È´òË¥®ÈáèÊñáÊú¨ÁîüÊàêÔºöÂ§ßÈáèËØÑ‰º∞ÂÆûÈ™åË°®ÊòéÔºåLLMs ËÉΩÂ§üÁîüÊàêÊµÅÁïÖ„ÄÅÂ§öÊ†∑„ÄÅÊñ∞È¢ñ„ÄÅÂèØÊéßÁöÑÊñáÊú¨„ÄÇÂ∞ΩÁÆ°Âú®‰∏™Âà´ËØ≠Ë®Ä‰∏äË°®Áé∞Ê¨†‰Ω≥Ôºå‰ΩÜÊï¥‰Ωì‰∏äÂÖ∑Â§áËâØÂ•ΩÁöÑÂ§öËØ≠Ë®ÄËÉΩÂäõ„ÄÇ</li><li>Ë®ÄÂ§ñ‰πãÊÑèÁöÑÁêÜËß£ÔºöÈô§‰∫ÜÁõ¥ËßÇË°®Áé∞Âá∫ÁöÑÂÜÖÂÆπÔºåËØ≠Ë®ÄËÉåÂêéÂèØËÉΩËøò‰º†ÈÄí‰∫ÜËØ¥ËØùËÄÖÁöÑÊÑèÂõæ„ÄÅÂÅèÂ•ΩÁ≠â‰ø°ÊÅØ„ÄÇË®ÄÂ§ñ‰πãÊÑèÊúâÂä©‰∫é‰ª£ÁêÜÊõ¥È´òÊïàÂú∞Ê≤üÈÄö‰∏éÂêà‰ΩúÔºåÂ§ßÊ®°ÂûãÂ∑≤ÁªèÂ±ïÁé∞Âá∫‰∫ÜËøôÊñπÈù¢ÁöÑÊΩúÂäõ„ÄÇ</li></ul><p><strong>Áü•ËØÜÔºö</strong>Âü∫‰∫éÂ§ßÊâπÈáèËØ≠ÊñôËÆ≠ÁªÉÁöÑ LLMsÔºåÊã•Êúâ‰∫ÜÂ≠òÂÇ®Êµ∑ÈáèÁü•ËØÜÔºàKnowledgeÔºâÁöÑËÉΩÂäõ„ÄÇÈô§‰∫ÜËØ≠Ë®ÄÁü•ËØÜ‰ª•Â§ñÔºåÂ∏∏ËØÜÁü•ËØÜÂíå‰∏ì‰∏öÊäÄËÉΩÁü•ËØÜÈÉΩÊòØ LLM-based Agents ÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜ„ÄÇ</p><p>ËôΩÁÑ∂ LLMs ÂÖ∂Êú¨Ë∫´‰ªçÁÑ∂Â≠òÂú®Áü•ËØÜËøáÊúü„ÄÅÂπªËßâÁ≠âÈóÆÈ¢òÔºåÁé∞ÊúâÁöÑ‰∏Ä‰∫õÁ†îÁ©∂ÈÄöËøáÁü•ËØÜÁºñËæëÊàñË∞ÉÁî®Â§ñÈÉ®Áü•ËØÜÂ∫ìÁ≠âÊñπÊ≥ïÔºåÂèØ‰ª•Âú®‰∏ÄÂÆöÁ®ãÂ∫¶‰∏äÂæóÂà∞ÁºìËß£„ÄÇ</p><p><strong>ËÆ∞ÂøÜÔºö</strong>Âú®Êú¨ÊñáÊ°ÜÊû∂‰∏≠ÔºåËÆ∞ÂøÜÊ®°ÂùóÔºàMemoryÔºâÂÇ®Â≠ò‰∫Ü‰ª£ÁêÜËøáÂæÄÁöÑËßÇÂØü„ÄÅÊÄùËÄÉÂíåË°åÂä®Â∫èÂàó„ÄÇÈÄöËøáÁâπÂÆöÁöÑËÆ∞ÂøÜÊú∫Âà∂Ôºå‰ª£ÁêÜÂèØ‰ª•ÊúâÊïàÂú∞ÂèçÊÄùÂπ∂Â∫îÁî®ÂÖàÂâçÁöÑÁ≠ñÁï•Ôºå‰ΩøÂÖ∂ÂÄüÈâ¥ËøáÂéªÁöÑÁªèÈ™åÊù•ÈÄÇÂ∫îÈôåÁîüÁöÑÁéØÂ¢É„ÄÇ</p><p>ÈÄöÂ∏∏Áî®‰∫éÊèêÂçáËÆ∞ÂøÜËÉΩÂäõÁöÑÊñπÊ≥ïÊúâ‰∏âÁßçÔºö</p><ul><li>Êâ©Â±ï Backbone Êû∂ÊûÑÁöÑÈïøÂ∫¶ÈôêÂà∂ÔºöÈíàÂØπ Transformers Âõ∫ÊúâÁöÑÂ∫èÂàóÈïøÂ∫¶ÈôêÂà∂ÈóÆÈ¢òËøõË°åÊîπËøõ„ÄÇ</li><li>ÊÄªÁªìËÆ∞ÂøÜÔºàSummarizingÔºâÔºöÂØπËÆ∞ÂøÜËøõË°åÊëòË¶ÅÊÄªÁªìÔºåÂ¢ûÂº∫‰ª£ÁêÜ‰ªéËÆ∞ÂøÜ‰∏≠ÊèêÂèñÂÖ≥ÈîÆÁªÜËäÇÁöÑËÉΩÂäõ„ÄÇ</li><li>ÂéãÁº©ËÆ∞ÂøÜÔºàCompressingÔºâÔºöÈÄöËøá‰ΩøÁî®ÂêëÈáèÊàñÈÄÇÂΩìÁöÑÊï∞ÊçÆÁªìÊûÑÂØπËÆ∞ÂøÜËøõË°åÂéãÁº©ÔºåÂèØ‰ª•ÊèêÈ´òËÆ∞ÂøÜÊ£ÄÁ¥¢ÊïàÁéá„ÄÇ</li></ul><p>Ê≠§Â§ñÔºåËÆ∞ÂøÜÁöÑÊ£ÄÁ¥¢ÊñπÊ≥ï‰πüÂæàÈáçË¶ÅÔºåÂè™ÊúâÊ£ÄÁ¥¢Âà∞ÂêàÈÄÇÁöÑÂÜÖÂÆπÔºå‰ª£ÁêÜÊâçËÉΩÂ§üËÆøÈóÆÂà∞ÊúÄÁõ∏ÂÖ≥ÂíåÂáÜÁ°ÆÁöÑ‰ø°ÊÅØ„ÄÇ</p><p><strong>Êé®ÁêÜ &amp; ËßÑÂàíÔºö</strong>Êé®ÁêÜËÉΩÂäõÔºàReasoningÔºâÂØπ‰∫éÊô∫ËÉΩ‰ª£ÁêÜËøõË°åÂÜ≥Á≠ñ„ÄÅÂàÜÊûêÁ≠âÂ§çÊùÇ‰ªªÂä°ËÄåË®ÄËá≥ÂÖ≥ÈáçË¶Å„ÄÇÂÖ∑‰ΩìÂà∞ LLMs ‰∏äÔºåÂ∞±ÊòØ‰ª• ÊÄùÁª¥ÈìæÔºàChain-of-ThoughtÔºåCoTÔºâ ‰∏∫‰ª£Ë°®ÁöÑ‰∏ÄÁ≥ªÂàóÊèêÁ§∫ÊñπÊ≥ï„ÄÇËÄåËßÑÂàíÔºàPlanningÔºâÂàôÊòØÈù¢ÂØπÂ§ßÂûãÊåëÊàòÊó∂Â∏∏Áî®ÁöÑÁ≠ñÁï•„ÄÇÂÆÉÂ∏ÆÂä©‰ª£ÁêÜÁªÑÁªáÊÄùÁª¥„ÄÅËÆæÂÆöÁõÆÊ†áÂπ∂Á°ÆÂÆöÂÆûÁé∞Ëøô‰∫õÁõÆÊ†áÁöÑÊ≠•È™§„ÄÇÂú®ÂÖ∑‰ΩìÂÆûÁé∞‰∏≠ÔºåËßÑÂàíÂèØ‰ª•ÂåÖÂê´‰∏§‰∏™Ê≠•È™§Ôºö</p><ul><li>ËÆ°ÂàíÂà∂ÂÆöÔºàPlan FormulationÔºâÔºö‰ª£ÁêÜÂ∞ÜÂ§çÊùÇ‰ªªÂä°ÂàÜËß£‰∏∫Êõ¥Êòì‰∫éÁÆ°ÁêÜÁöÑÂ≠ê‰ªªÂä°„ÄÇ‰æãÂ¶ÇÔºö‰∏ÄÊ¨°ÊÄßÂàÜËß£ÂÜçÊåâÈ°∫Â∫èÊâßË°å„ÄÅÈÄêÊ≠•ËßÑÂàíÂπ∂ÊâßË°å„ÄÅÂ§öË∑ØËßÑÂàíÂπ∂ÈÄâÂèñÊúÄ‰ºòË∑ØÂæÑÁ≠â„ÄÇÂú®‰∏Ä‰∫õÈúÄË¶Å‰∏ì‰∏öÁü•ËØÜÁöÑÂú∫ÊôØ‰∏≠Ôºå‰ª£ÁêÜÂèØ‰∏éÁâπÂÆöÈ¢ÜÂüüÁöÑ Planner Ê®°ÂùóÈõÜÊàêÔºåÊèêÂçáËÉΩÂäõ„ÄÇ</li><li>ËÆ°ÂàíÂèçÊÄùÔºàPlan ReflectionÔºâÔºöÂú®Âà∂ÂÆöËÆ°ÂàíÂêéÔºåÂèØ‰ª•ËøõË°åÂèçÊÄùÂπ∂ËØÑ‰º∞ÂÖ∂‰ºòÂä£„ÄÇËøôÁßçÂèçÊÄù‰∏ÄËà¨Êù•Ëá™‰∏â‰∏™ÊñπÈù¢ÔºöÂÄüÂä©ÂÜÖÈÉ®ÂèçÈ¶àÊú∫Âà∂Ôºõ‰∏é‰∫∫Á±ª‰∫íÂä®Ëé∑ÂæóÂèçÈ¶àÔºõ‰ªéÁéØÂ¢É‰∏≠Ëé∑ÂæóÂèçÈ¶à„ÄÇ</li></ul><p><strong>ËøÅÁßªÊÄß &amp; Ê≥õÂåñÊÄßÔºö</strong>Êã•Êúâ‰∏ñÁïåÁü•ËØÜÁöÑ LLMs Ëµã‰∫àÊô∫ËÉΩ‰ª£ÁêÜÂÖ∑Â§áÂº∫Â§ßÁöÑËøÅÁßª‰∏éÊ≥õÂåñËÉΩÂäõ„ÄÇ‰∏Ä‰∏™Â•ΩÁöÑ‰ª£ÁêÜ‰∏çÊòØÈùôÊÄÅÁöÑÁü•ËØÜÂ∫ìÔºåËøòÂ∫îÂÖ∑Â§áÂä®ÊÄÅÁöÑÂ≠¶‰π†ËÉΩÂäõÔºö</p><ul><li>ÂØπÊú™Áü•‰ªªÂä°ÁöÑÊ≥õÂåñÔºöÈöèÁùÄÊ®°ÂûãËßÑÊ®°‰∏éËÆ≠ÁªÉÊï∞ÊçÆÁöÑÂ¢ûÂ§ßÔºåLLMs Âú®Ëß£ÂÜ≥Êú™Áü•‰ªªÂä°‰∏äÊ∂åÁé∞Âá∫‰∫ÜÊÉä‰∫∫ÁöÑËÉΩÂäõ„ÄÇÈÄöËøáÊåá‰ª§ÂæÆË∞ÉÁöÑÂ§ßÊ®°ÂûãÂú® zero-shot ÊµãËØï‰∏≠Ë°®Áé∞ËâØÂ•ΩÔºåÂú®ËÆ∏Â§ö‰ªªÂä°‰∏äÈÉΩÂèñÂæó‰∫Ü‰∏ç‰∫ö‰∫é‰∏ìÂÆ∂Ê®°ÂûãÁöÑÊàêÁª©„ÄÇ</li><li>ÊÉÖÊôØÂ≠¶‰π†ÔºàIn-context LearningÔºâÔºöÂ§ßÊ®°Âûã‰∏ç‰ªÖËÉΩÂ§ü‰ªé‰∏ä‰∏ãÊñáÁöÑÂ∞ëÈáèÁ§∫‰æã‰∏≠ËøõË°åÁ±ªÊØîÂ≠¶‰π†ÔºåËøôÁßçËÉΩÂäõËøòÂèØ‰ª•Êâ©Â±ïÂà∞ÊñáÊú¨‰ª•Â§ñÁöÑÂ§öÊ®°ÊÄÅÂú∫ÊôØÔºå‰∏∫‰ª£ÁêÜÂú®Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂ∫îÁî®Êèê‰æõ‰∫ÜÊõ¥Â§öÂèØËÉΩÊÄß„ÄÇ</li><li>ÊåÅÁª≠Â≠¶‰π†ÔºàContinual LearningÔºâÔºöÊåÅÁª≠Â≠¶‰π†ÁöÑ‰∏ªË¶ÅÊåëÊàòÊòØÁÅæÈöæÊÄßÈÅóÂøòÔºåÂç≥ÂΩìÊ®°ÂûãÂ≠¶‰π†Êñ∞‰ªªÂä°Êó∂ÂÆπÊòì‰∏¢Â§±ËøáÂæÄ‰ªªÂä°‰∏≠ÁöÑÁü•ËØÜ„ÄÇ‰∏ìÊúâÈ¢ÜÂüüÁöÑÊô∫ËÉΩ‰ª£ÁêÜÂ∫îÂΩìÂ∞ΩÈáèÈÅøÂÖç‰∏¢Â§±ÈÄöÁî®È¢ÜÂüüÁöÑÁü•ËØÜ„ÄÇ</li></ul><p>####ÊÑüÁü•Á´ØÔºöPerception</p><p>‰∫∫Á±ªÈÄöËøáÂ§öÊ®°ÊÄÅÁöÑÊñπÂºèÊÑüÁü•‰∏ñÁïåÔºåÊâÄ‰ª•Á†îÁ©∂ËÄÖ‰ª¨ÂØπ LLM-based Agents Êä±ÊúâÂêåÊ†∑ÁöÑÊúüÂæÖ„ÄÇÂ§öÊ®°ÊÄÅÊÑüÁü•ËÉΩÂä†Ê∑±‰ª£ÁêÜÂØπÂ∑•‰ΩúÁéØÂ¢ÉÁöÑÁêÜËß£ÔºåÊòæËëóÊèêÂçá‰∫ÜÂÖ∂ÈÄöÁî®ÊÄß„ÄÇ</p><p><strong>ÊñáÊú¨ËæìÂÖ•Ôºö</strong>‰Ωú‰∏∫ LLMs ÊúÄÂü∫Á°ÄÁöÑËÉΩÂäõÔºåËøôÈáå‰∏çÂÜçËµòËø∞„ÄÇ</p><p><strong>ËßÜËßâËæìÂÖ•Ôºö</strong>LLMs Êú¨Ë∫´Âπ∂‰∏çÂÖ∑Â§áËßÜËßâÁöÑÊÑüÁü•ËÉΩÂäõÔºåÂè™ËÉΩÁêÜËß£Á¶ªÊï£ÁöÑÊñáÊú¨ÂÜÖÂÆπ„ÄÇËÄåËßÜËßâËæìÂÖ•ÈÄöÂ∏∏ÂåÖÂê´ÊúâÂÖ≥‰∏ñÁïåÁöÑÂ§ßÈáè‰ø°ÊÅØÔºåÂåÖÊã¨ÂØπË±°ÁöÑÂ±ûÊÄßÔºåÁ©∫Èó¥ÂÖ≥Á≥ªÔºåÂú∫ÊôØÂ∏ÉÂ±ÄÁ≠âÁ≠â„ÄÇÂ∏∏ËßÅÁöÑÊñπÊ≥ïÊúâÔºö</p><ul><li>Â∞ÜËßÜËßâËæìÂÖ•ËΩ¨‰∏∫ÂØπÂ∫îÁöÑÊñáÊú¨ÊèèËø∞ÔºàImage CaptioningÔºâÔºöÂèØ‰ª•Ë¢´ LLMs Áõ¥Êé•ÁêÜËß£ÔºåÂπ∂‰∏îÂèØËß£ÈáäÊÄßÈ´ò„ÄÇ</li><li>ÂØπËßÜËßâ‰ø°ÊÅØËøõË°åÁºñÁ†ÅË°®Á§∫Ôºö‰ª•ËßÜËßâÂü∫Á°ÄÊ®°Âûã + LLMs ÁöÑËåÉÂºèÊù•ÊûÑÊàêÊÑüÁü•Ê®°ÂùóÔºåÈÄöËøáÂØπÈΩêÊìç‰ΩúÊù•ËÆ©Ê®°ÂûãÁêÜËß£‰∏çÂêåÊ®°ÊÄÅÁöÑÂÜÖÂÆπÔºåÂèØ‰ª•Á´ØÂà∞Á´ØÁöÑÊñπÂºèËøõË°åËÆ≠ÁªÉ„ÄÇ</li></ul><p><strong>Âê¨ËßâËæìÂÖ•Ôºö</strong>Âê¨Ëßâ‰πüÊòØ‰∫∫Á±ªÊÑüÁü•‰∏≠ÁöÑÈáçË¶ÅÁªÑÊàêÈÉ®ÂàÜ„ÄÇÁî±‰∫é LLMs ÊúâÁùÄ‰ºòÁßÄÁöÑÂ∑•ÂÖ∑Ë∞ÉÁî®ËÉΩÂäõÔºå‰∏Ä‰∏™Áõ¥ËßÇÁöÑÊÉ≥Ê≥ïÂ∞±ÊòØÔºö‰ª£ÁêÜÂèØ‰ª•Â∞Ü LLMs ‰Ωú‰∏∫ÊéßÂà∂Êû¢Á∫ΩÔºåÈÄöËøáÁ∫ßËÅîÁöÑÊñπÂºèË∞ÉÁî®Áé∞ÊúâÁöÑÂ∑•ÂÖ∑ÈõÜÊàñËÄÖ‰∏ìÂÆ∂Ê®°ÂûãÔºåÊÑüÁü•Èü≥È¢ë‰ø°ÊÅØ„ÄÇÊ≠§Â§ñÔºåÈü≥È¢ë‰πüÂèØ‰ª•ÈÄöËøáÈ¢ëË∞±ÂõæÔºàSpectrogramÔºâÁöÑÊñπÂºèËøõË°åÁõ¥ËßÇË°®Á§∫„ÄÇÈ¢ëË∞±ÂõæÂèØ‰ª•‰Ωú‰∏∫Âπ≥Èù¢ÂõæÂÉèÊù•Â±ïÁ§∫ 2D ‰ø°ÊÅØÔºåÂõ†Ê≠§Ôºå‰∏Ä‰∫õËßÜËßâÁöÑÂ§ÑÁêÜÊñπÊ≥ïÂèØ‰ª•ËøÅÁßªÂà∞ËØ≠Èü≥È¢ÜÂüü„ÄÇ</p><p><strong>ÂÖ∂‰ªñËæìÂÖ•Ôºö</strong>Áé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑ‰ø°ÊÅØËøú‰∏çÊ≠¢ÊñáÊú¨„ÄÅËßÜËßâÂíåÂê¨Ëßâ„ÄÇ‰ΩúËÄÖ‰ª¨Â∏åÊúõÂú®Êú™Êù•ÔºåÊô∫ËÉΩ‰ª£ÁêÜËÉΩÈÖçÂ§áÊõ¥‰∏∞ÂØåÁöÑÊÑüÁü•Ê®°ÂùóÔºå‰æãÂ¶ÇËß¶Ëßâ„ÄÅÂóÖËßâÁ≠âÂô®ÂÆòÔºåÁî®‰∫éËé∑ÂèñÁõÆÊ†áÁâ©‰ΩìÊõ¥Âä†‰∏∞ÂØåÁöÑÂ±ûÊÄß„ÄÇÂêåÊó∂Ôºå‰ª£ÁêÜ‰πüËÉΩÂØπÂë®Âõ¥ÁéØÂ¢ÉÁöÑÊ∏©Â∫¶„ÄÅÊπøÂ∫¶ÂíåÊòéÊöóÁ®ãÂ∫¶ÊúâÊ∏ÖÊ•öÁöÑÊÑüÂèóÔºåÈááÂèñÊõ¥ Environment-aware ÁöÑË°åÂä®„ÄÇ</p><p>Ê≠§Â§ñÔºåËøòÂèØ‰ª•‰∏∫‰ª£ÁêÜÂºïÂÖ•ÂØπÊõ¥ÂπøÈòîÁöÑÊï¥‰ΩìÁéØÂ¢ÉÁöÑÊÑüÁü•ÔºöÈááÁî®ÊøÄÂÖâÈõ∑Ëææ„ÄÅGPS„ÄÅÊÉØÊÄßÊµãÈáèÂçïÂÖÉÁ≠âÊàêÁÜüÁöÑÊÑüÁü•Ê®°Âùó„ÄÇ</p><h4 id="Ë°åÂä®Á´ØÔºöAction"><a href="#Ë°åÂä®Á´ØÔºöAction" class="headerlink" title="Ë°åÂä®Á´ØÔºöAction"></a><strong>Ë°åÂä®Á´ØÔºöAction</strong></h4><p>Âú®Â§ßËÑëÂÅöÂá∫ÂàÜÊûê„ÄÅÂÜ≥Á≠ñÂêéÔºå‰ª£ÁêÜËøòÈúÄË¶ÅÂÅöÂá∫Ë°åÂä®‰ª•ÈÄÇÂ∫îÊàñÊîπÂèòÁéØÂ¢ÉÔºö</p><p><strong>ÊñáÊú¨ËæìÂá∫Ôºö</strong>‰Ωú‰∏∫ LLMs ÊúÄÂü∫Á°ÄÁöÑËÉΩÂäõÔºåËøôÈáå‰∏çÂÜçËµòËø∞„ÄÇ</p><p><strong>Â∑•ÂÖ∑‰ΩøÁî®Ôºö</strong>Â∞ΩÁÆ° LLMs Êã•ÊúâÂá∫Ëâ≤ÁöÑÁü•ËØÜÂÇ®Â§áÂíå‰∏ì‰∏öËÉΩÂäõÔºå‰ΩÜÂú®Èù¢ÂØπÂÖ∑‰ΩìÈóÆÈ¢òÊó∂Ôºå‰πüÂèØËÉΩ‰ºöÂá∫Áé∞È≤ÅÊ£íÊÄßÈóÆÈ¢ò„ÄÅÂπªËßâÁ≠â‰∏ÄÁ≥ªÂàóÊåëÊàò„ÄÇ‰∏éÊ≠§ÂêåÊó∂ÔºåÂ∑•ÂÖ∑‰Ωú‰∏∫‰ΩøÁî®ËÄÖËÉΩÂäõÁöÑÊâ©Â±ïÔºåÂèØ‰ª•Âú®‰∏ì‰∏öÊÄß„ÄÅ‰∫ãÂÆûÊÄß„ÄÅÂèØËß£ÈáäÊÄßÁ≠âÊñπÈù¢Êèê‰æõÂ∏ÆÂä©„ÄÇ‰æãÂ¶ÇÔºåÂèØ‰ª•ÈÄöËøá‰ΩøÁî®ËÆ°ÁÆóÂô®Êù•ËÆ°ÁÆóÊï∞Â≠¶ÈóÆÈ¢ò„ÄÅ‰ΩøÁî®ÊêúÁ¥¢ÂºïÊìéÊù•ÊêúÂØªÂÆûÊó∂‰ø°ÊÅØ„ÄÇ</p><p>Âè¶Â§ñÔºåÂ∑•ÂÖ∑‰πüÂèØ‰ª•Êâ©Â±ïÊô∫ËÉΩ‰ª£ÁêÜÁöÑË°åÂä®Á©∫Èó¥„ÄÇ‰æãÂ¶ÇÔºåÈÄöËøáË∞ÉÁî®ËØ≠Èü≥ÁîüÊàê„ÄÅÂõæÂÉèÁîüÊàêÁ≠â‰∏ìÂÆ∂Ê®°ÂûãÔºåÊù•Ëé∑ÂæóÂ§öÊ®°ÊÄÅÁöÑË°åÂä®ÊñπÂºè„ÄÇÂõ†Ê≠§ÔºåÂ¶Ç‰ΩïËÆ©‰ª£ÁêÜÊàê‰∏∫‰ºòÁßÄÁöÑÂ∑•ÂÖ∑‰ΩøÁî®ËÄÖÔºåÂç≥Â≠¶‰ºöÂ¶Ç‰ΩïÊúâÊïàÂú∞Âà©Áî®Â∑•ÂÖ∑ÔºåÊòØÈùûÂ∏∏ÈáçË¶Å‰∏îÊúâÂâçÊôØÁöÑÊñπÂêë„ÄÇ</p><p>ÁõÆÂâçÔºå‰∏ªË¶ÅÁöÑÂ∑•ÂÖ∑Â≠¶‰π†ÊñπÊ≥ïÂåÖÊã¨‰ªéÊºîÁ§∫‰∏≠Â≠¶‰π†Âíå‰ªéÂèçÈ¶à‰∏≠Â≠¶‰π†„ÄÇÊ≠§Â§ñÔºå‰πüÂèØ‰ª•ÈÄöËøáÂÖÉÂ≠¶‰π†„ÄÅËØæÁ®ãÂ≠¶‰π†Á≠âÊñπÂºèÊù•ËÆ©‰ª£ÁêÜÁ®ãÂ∫èÂú®‰ΩøÁî®ÂêÑÁßçÂ∑•ÂÖ∑ÊñπÈù¢ÂÖ∑Â§áÊ≥õÂåñËÉΩÂäõ„ÄÇÊõ¥Ëøõ‰∏ÄÊ≠•ÔºåÊô∫ËÉΩ‰ª£ÁêÜËøòÂèØ‰ª•Ëøõ‰∏ÄÊ≠•Â≠¶‰π†Â¶Ç‰Ωï„ÄåËá™ÁªôËá™Ë∂≥„ÄçÂú∞Âà∂ÈÄ†Â∑•ÂÖ∑Ôºå‰ªéËÄåÊèêÈ´òÂÖ∂Ëá™‰∏ªÊÄßÂíåÁã¨Á´ãÊÄß„ÄÇ</p><p><strong>ÂÖ∑Ë∫´Ë°åÂä®Ôºö</strong>ÂÖ∑Ë∫´ÔºàEmbodymentÔºâÊòØÊåá‰ª£ÁêÜ‰∏éÁéØÂ¢É‰∫§‰∫íËøáÁ®ã‰∏≠ÔºåÁêÜËß£„ÄÅÊîπÈÄ†ÁéØÂ¢ÉÂπ∂Êõ¥Êñ∞Ëá™Ë∫´Áä∂ÊÄÅÁöÑËÉΩÂäõ„ÄÇÂÖ∑Ë∫´Ë°åÂä®ÔºàEmbodied ActionÔºâË¢´ËßÜ‰∏∫ËôöÊãüÊô∫ËÉΩ‰∏éÁâ©ÁêÜÁé∞ÂÆûÁöÑ‰∫íÈÄöÊ°•Ê¢Å„ÄÇ</p><p>‰º†ÁªüÁöÑÂü∫‰∫éÂº∫ÂåñÂ≠¶‰π†ÁöÑ Agent Âú®Ê†∑Êú¨ÊïàÁéá„ÄÅÊ≥õÂåñÊÄßÂíåÂ§çÊùÇÈóÆÈ¢òÊé®ÁêÜÁ≠âÊñπÈù¢Â≠òÂú®Â±ÄÈôêÊÄßÔºåËÄå LLM-based Agents ÈÄöËøáÂºïÂÖ•Â§ßÊ®°Âûã‰∏∞ÂØåÁöÑÂÜÖÂú®Áü•ËØÜÔºå‰ΩøÂæó Embodied Agent ËÉΩÂ§üÂÉè‰∫∫Á±ª‰∏ÄÊ†∑‰∏ªÂä®ÊÑüÁü•„ÄÅÂΩ±ÂìçÁâ©ÁêÜÁéØÂ¢É„ÄÇÊ†πÊçÆ‰ª£ÁêÜÂú®‰ªªÂä°‰∏≠ÁöÑËá™‰∏ªÁ®ãÂ∫¶ÊàñËÄÖËØ¥ Action ÁöÑÂ§çÊùÇÁ®ãÂ∫¶ÔºåÂèØ‰ª•Êúâ‰ª•‰∏ãÁöÑÂéüÂ≠ê ActionÔºö</p><ul><li>Observation ÂèØ‰ª•Â∏ÆÂä©Êô∫ËÉΩ‰ª£ÁêÜÂú®ÁéØÂ¢É‰∏≠ÂÆö‰ΩçËá™Ë∫´‰ΩçÁΩÆ„ÄÅÊÑüÁü•ÂØπË±°Áâ©ÂìÅÂíåËé∑ÂèñÂÖ∂‰ªñÁéØÂ¢É‰ø°ÊÅØÔºõ</li><li>Manipulation ÂàôÊòØÂÆåÊàê‰∏Ä‰∫õÂÖ∑‰ΩìÁöÑÊäìÂèñ„ÄÅÊé®Âä®Á≠âÊìç‰Ωú‰ªªÂä°Ôºõ</li><li>Navigation Ë¶ÅÊ±ÇÊô∫ËÉΩ‰ª£ÁêÜÊ†πÊçÆ‰ªªÂä°ÁõÆÊ†áÂèòÊç¢Ëá™Ë∫´‰ΩçÁΩÆÂπ∂Ê†πÊçÆÁéØÂ¢É‰ø°ÊÅØÊõ¥Êñ∞Ëá™Ë∫´Áä∂ÊÄÅ„ÄÇ</li></ul><p>ÈÄöËøáÁªÑÂêàËøô‰∫õÂéüÂ≠êË°åÂä®Ôºå‰ª£ÁêÜÂèØ‰ª•ÂÆåÊàêÊõ¥‰∏∫Â§çÊùÇÁöÑ‰ªªÂä°„ÄÇ‰æãÂ¶Ç„ÄåÂé®ÊàøÁöÑË•øÁìúÊØîÁ¢óÂ§ßÂêóÔºü„ÄçËøôÁ±ªÂÖ∑Ë∫´ÁöÑ QA ‰ªªÂä°„ÄÇ‰∏∫‰∫ÜËß£ÂÜ≥Ëøô‰∏™ÈóÆÈ¢òÔºå‰ª£ÁêÜÈúÄË¶ÅÂØºËà™Âà∞Âé®ÊàøÔºåÂπ∂Âú®ËßÇÂØü‰∫åËÄÖÁöÑÂ§ßÂ∞èÂêéÂæóÂá∫Á≠îÊ°à„ÄÇ</p><p>ÂèóÈôê‰∫éÁâ©ÁêÜ‰∏ñÁïåÁ°¨‰ª∂ÁöÑÈ´òÊàêÊú¨ÂíåÂÖ∑Ë∫´Êï∞ÊçÆÈõÜÁº∫‰πèÁ≠âÈóÆÈ¢òÔºåÁõÆÂâçÂÖ∑Ë∫´Ë°åÂä®ÁöÑÁ†îÁ©∂‰ªç‰∏ªË¶ÅÈõÜ‰∏≠‰∫éÊ∏∏ÊàèÂπ≥Âè∞„ÄäÊàëÁöÑ‰∏ñÁïå„ÄãÁ≠âËôöÊãüÊ≤ôÁõíÁéØÂ¢É‰∏≠„ÄÇÂõ†Ê≠§Ôºå‰∏ÄÊñπÈù¢‰ΩúËÄÖ‰ª¨ÊúüÂæÖÊúâ‰∏ÄÁßçÊõ¥Ë¥¥ËøëÁé∞ÂÆûÁöÑ‰ªªÂä°ËåÉÂºèÂíåËØÑ‰ª∑Ê†áÂáÜÔºåÂè¶‰∏ÄÊñπÈù¢Ôºå‰πüÈúÄË¶ÅÂ§ßÂÆ∂Âú®È´òÊïàÊûÑÂª∫Áõ∏ÂÖ≥Êï∞ÊçÆÈõÜ‰∏äÈù¢ÊúâÊõ¥Â§öÁöÑÊé¢Á¥¢„ÄÇ</p><h3 id="Áî®Êï∞ÊçÆÈõÜÊ≥®ÂÖ•Agent-ËÉΩÂäõ‚Äî‚ÄîAgent-tuning"><a href="#Áî®Êï∞ÊçÆÈõÜÊ≥®ÂÖ•Agent-ËÉΩÂäõ‚Äî‚ÄîAgent-tuning" class="headerlink" title="Áî®Êï∞ÊçÆÈõÜÊ≥®ÂÖ•Agent ËÉΩÂäõ‚Äî‚ÄîAgent tuning"></a>Áî®Êï∞ÊçÆÈõÜÊ≥®ÂÖ•Agent ËÉΩÂäõ‚Äî‚ÄîAgent tuning</h3><p><img src="/img/image-20231104204218581.png" alt="image-20231104204218581"></p><p>ËÆ∫ÊñáËÉåÊôØ: Â§ßÂûãËØ≠Ë®ÄÊ®°ÂûãÔºàLLMsÔºâÂú®ÂêÑÁßç‰ªªÂä°‰∏≠Ë°®Áé∞Âá∫Ëâ≤Ôºå‰ΩÜÂú®‰Ωú‰∏∫‰ª£ÁêÜÂ§ÑÁêÜÁé∞ÂÆû‰∏ñÁïå‰∏≠ÁöÑÂ§çÊùÇ‰ªªÂä°Êó∂Ôºå‰∏éÂïÜ‰∏öÊ®°ÂûãÔºàÂ¶ÇChatGPTÂíåGPT-4ÔºâÁõ∏ÊØî‰ªçÁÑ∂Â≠òÂú®Â∑ÆË∑ù„ÄÇ</p><p>ËøáÂéªÁöÑÁ†îÁ©∂‰∏ªË¶ÅÈõÜ‰∏≠Âú®ËÆæËÆ°ÁâπÂÆö‰ª£ÁêÜ‰ªªÂä°ÁöÑÊèêÁ§∫ÊàñÊ°ÜÊû∂ÔºåËÄåÁº∫‰πèÂØπLLMs‰ª£ÁêÜËÉΩÂäõÊú¨Ë∫´ÁöÑÊîπËøõÁ†îÁ©∂„ÄÇ</p><p>ËÆ∫ÊñáÁöÑMotivation: ‰∏∫‰∫ÜÂÆûÁé∞LLMsÁöÑÂπø‰πâ‰ª£ÁêÜËÉΩÂäõÔºåÊú¨Á†îÁ©∂ÊèêÂá∫‰∫ÜAgentTuningÊñπÊ≥ïÔºåÈÄöËøáÊûÑÂª∫AgentInstructÊï∞ÊçÆÈõÜÂíåÊ∑∑ÂêàÊåá‰ª§Ë∞ÉÊï¥Á≠ñÁï•ÔºåÊó®Âú®Â¢ûÂº∫LLMsÁöÑ‰ª£ÁêÜËÉΩÂäõÔºåÂêåÊó∂‰øùÊåÅÂÖ∂Âπø‰πâËÉΩÂäõ„ÄÇÈÄöËøáÂÆûÈ™åËØÅÊòéÔºåAgentTuning‰ΩøÂæóLLMsÂú®‰ª£ÁêÜ‰ªªÂä°‰∏äÂÖ∑ÊúâÂπø‰πâËÉΩÂäõÔºåÂπ∂‰∏îÂú®Êú™ËßÅËøáÁöÑ‰ª£ÁêÜ‰ªªÂä°‰∏ä‰∏éGPT-3.5Áõ∏Â™≤Áæé„ÄÇ </p><h5 id="overview"><a href="#overview" class="headerlink" title="overview"></a>overview</h5><p><img src="https://p1k05ba99n.feishu.cn/space/api/box/stream/download/asynccode/?code=Yzg0M2M4YTk4ZGQyMDlkOWQ2ZDNkMjU1OTYzZGNiNGZfelY5cVJoWVRhNmJ4ZnUzQ2RuMkYwRENIRTJCY09QaERfVG9rZW46QnNyeGI4RnVSb3BZbEN4V05GemNwNkRybnVlXzE2OTkxMDE4Mzk6MTY5OTEwNTQzOV9WNA" alt="img"></p><h5 id="instruction-generation"><a href="#instruction-generation" class="headerlink" title="instruction generation"></a>instruction generation</h5><p><img src="https://p1k05ba99n.feishu.cn/space/api/box/stream/download/asynccode/?code=MDJkZTE3MTk1NTY1M2M4NjhhZjg1YTYzNjE5YzAxYmFfNlJBU1JGNWNDYk0yUXhUOU5DalNwMG1hcWFUUVI3RThfVG9rZW46TEtJTWJIR3VNb0xqTXd4bWtHWmNDbFJTbmtnXzE2OTkxMDE4Mzk6MTY5OTEwNTQzOV9WNA" alt="img"></p><p>‰∏äË°®Â±ïÁ§∫‰∫Ü‰ΩøÁî®ÁöÑÊï∞ÊçÆÈõÜÔºåÂ¶ÇÊûú‰∏Ä‰∏™‰ªªÂä°(‰æãÂ¶Ç ALFWorld„ÄÅ WebShop„ÄÅ Mind2Web Âíå Knowledge Graph)Êúâ‰∏Ä‰∏™ËÆ≠ÁªÉÈõÜÔºåÊú¨ÊñáÁõ¥Êé•‰ΩøÁî®training splitËøõË°åÂêéÁª≠ÁöÑÈò∂ÊÆµ„ÄÇÂØπ‰∫éÊ≤°ÊúâËÆ≠ÁªÉÈõÜÁöÑÊìç‰ΩúÁ≥ªÁªüÂíåÊï∞ÊçÆÂ∫ì‰ªªÂä°ÔºåÊú¨ÊñáÂà©Áî®Task DerivationÂíåSelf-InstructÁöÑÊÄùÊÉ≥(Wang et al„ÄÇ Ôºå2023c)Êù•ÊûÑÂª∫Áõ∏Â∫îÁöÑÊåá‰ª§„ÄÇÔºö</p><ol><li><strong>Task Derivation</strong> ÂØπ‰∫é‰∏éÂ∑≤ÁªèË¢´ÂπøÊ≥õÁ†îÁ©∂ÁöÑÂú∫ÊôØÁõ∏ÂÖ≥ÁöÑ‰ª£ÁêÜ‰ªªÂä°ÔºåÊú¨ÊñáÂèØ‰ª•Áõ¥Êé•‰ªéÁõ∏‰ººÁöÑÊï∞ÊçÆÈõÜÊûÑÈÄ†Êåá‰ª§„ÄÇÂõ†Ê≠§Ôºå‰∏∫‰∫ÜÂú®Êï∞ÊçÆÂ∫ì(DB)‰ªªÂä°‰∏äÊûÑÈÄ†Êåá‰ª§ÔºåÊú¨Êñá‰ªé BIRD (Li et al„ÄÇ Ôºå2023)Ëé∑ÂæóÊåá‰ª§ÔºåBIRD ÊòØ‰∏Ä‰∏™Âè™ÊîØÊåÅ SELECT ÁöÑÊï∞ÊçÆÂ∫ìÂü∫ÂáÜ„ÄÇÊú¨ÊñáËøêË°å‰∫Ü‰∏§ÁßçÁ±ªÂûãÁöÑ‰ªªÂä°Ê¥æÁîü„ÄÇÈ¶ñÂÖàÔºåÊú¨Êñá‰ΩøÁî®ÈóÆÈ¢òÂíåÂèÇËÄÉ SQL ËØ≠Âè•Âú®ÊØè‰∏™ BIRD Â≠ê‰ªªÂä°‰∏≠ÊûÑÈÄ†‰∏Ä‰∏™ËΩ®Ëøπ„ÄÇÁÑ∂ÂêéÔºåÊú¨Êñá‰ΩøÁî®ÂºïÁî® SQL ËØ≠Âè•Êü•ËØ¢Êï∞ÊçÆÂ∫ìÔºå‰ª•Ëé∑ÂæóÊï∞ÊçÆÂ∫ìÁöÑËæìÂá∫ÔºåÂπ∂Â∞ÜÂÖ∂‰Ωú‰∏∫‰ª£ÁêÜÊèê‰∫§ÁöÑÁ≠îÊ°à„ÄÇÊúÄÂêéÔºåÊú¨ÊñáË¶ÅÊ±Ç GPT-4Â°´ÂÜôÁªôÂá∫‰∏äËø∞‰ø°ÊÅØÁöÑ‰ª£ÁêÜÁöÑÊÉ≥Ê≥ï„ÄÇËøôÊ†∑ÔºåÊú¨ÊñáÂèØ‰ª•Áõ¥Êé•‰ªé BIRD Êï∞ÊçÆÈõÜÁîüÊàêÊ≠£Á°ÆÁöÑËΩ®Ëøπ„ÄÇ</li><li><strong>Self-Instruct</strong> ÂØπ‰∫éÊìç‰ΩúÁ≥ªÁªü(OS)‰ªªÂä°ÔºåÁî±‰∫éÈöæ‰ª•Ëé∑ÂæóÊ∂âÂèäÊìç‰ΩúÁªàÁ´ØÊìç‰ΩúÁ≥ªÁªüÁöÑÊåá‰ª§ÔºåÊú¨ÊñáÈááÁî®‰∫ÜËá™Êåá‰ª§ÊñπÊ≥ï(Wang et al„ÄÇ Ôºå2023c)Êù•ÊûÑÂª∫‰ªªÂä°„ÄÇÊú¨ÊñáÈ¶ñÂÖàÊèêÁ§∫ GPT-4ÊèêÂá∫‰∏Ä‰∫õ‰∏éÊìç‰ΩúÁ≥ªÁªüÁõ∏ÂÖ≥ÁöÑ‰ªªÂä°Ôºå‰ª•ÂèäÂØπ‰ªªÂä°ÁöÑËß£Èáä„ÄÅÂèÇËÄÉËß£ÂÜ≥ÊñπÊ°àÂíåËØÑ‰º∞ËÑöÊú¨„ÄÇÁÑ∂ÂêéÔºåÊú¨ÊñáÁî®‰ªªÂä°ÊèêÁ§∫Âè¶‰∏Ä‰∏™ GPT-4ÂÆû‰æã(Ê±ÇËß£Âô®)Âπ∂Êî∂ÈõÜÂÆÉÁöÑËΩ®Ëøπ„ÄÇ‰ªªÂä°ÂÆåÊàêÂêéÔºåÊú¨ÊñáËøêË°åÂèÇËÄÉËß£ÂÜ≥ÊñπÊ°àÔºåÂπ∂‰ΩøÁî®ËØÑ‰º∞ËÑöÊú¨Â∞ÜÂÖ∂ÁªìÊûú‰∏éÊù•Ëá™Ê±ÇËß£Âô® GPT-4ÁöÑÁªìÊûúËøõË°åÊØîËæÉ„ÄÇÊú¨ÊñáÊî∂ÈõÜÁöÑËΩ®ËøπÔºåÂÖ∂‰∏≠ÁöÑÂèÇËÄÉËß£ÂÜ≥ÊñπÊ°àÂíåËß£ÂÜ≥ÊñπÊ°àÁªôÂá∫‰∫ÜÁõ∏ÂêåÁöÑÁ≠îÊ°à„ÄÇÂØπ‰∫é DB ‰ªªÂä°ÔºåÁî±‰∫é BIRD Âè™ÂåÖÂê´ SELECT Êï∞ÊçÆÔºåÊú¨Êñá‰ΩøÁî®Á±ª‰ººÁöÑËá™Êåá‰ª§ÊñπÊ≥ïÊûÑÈÄ†ÂÖ∂‰ªñÁ±ªÂûãÁöÑÊï∞ÊçÆÂ∫ìÊìç‰Ωú(INSERT„ÄÅ UPDATE Âíå DELETE)„ÄÇ</li></ol><h5 id="trajectory-interaction"><a href="#trajectory-interaction" class="headerlink" title="trajectory interaction"></a>trajectory interaction</h5><ol><li><strong>Interaction Process</strong> ‰∫§‰∫íËøáÁ®ã‰∫§‰∫íËøáÁ®ãÊúâ‰∏§‰∏™‰∏ªË¶ÅÈÉ®ÂàÜ„ÄÇÈ¶ñÂÖàÔºåÊú¨ÊñáÁªôÂá∫‰∫ÜÊ®°ÂûãÁöÑ‰ªªÂä°ÊèèËø∞Âíå‰∏Ä‰∏™ÊàêÂäüÁöÑ‰∏ÄÊùÜÁöÑ‰æãÂ≠ê„ÄÇÁÑ∂ÂêéÔºåÁúüÊ≠£ÁöÑ‰∫íÂä®ÂºÄÂßã‰∫Ü„ÄÇÊú¨Êñá‰∏∫Ê®°ÂûãÊèê‰æõ‰∫ÜÂΩìÂâçÁöÑÊåá‰ª§ÂíåÂøÖË¶ÅÁöÑ‰ø°ÊÅØ„ÄÇÂü∫‰∫éËøô‰∏™Âíå‰ª•ÂâçÁöÑÂèçÈ¶àÔºåÊ®°ÂûãÂΩ¢Êàê‰∏Ä‰∏™ÊÄùÊÉ≥Âπ∂ÈááÂèñË°åÂä®„ÄÇÁÑ∂ÂêéÁéØÂ¢ÉÊèê‰æõÂèçÈ¶àÔºåÂåÖÊã¨ÂèØËÉΩÁöÑÊõ¥ÊîπÊàñÊñ∞‰ø°ÊÅØ„ÄÇËøô‰∏™Âæ™ÁéØ‰∏ÄÁõ¥ÊåÅÁª≠Âà∞Ê®°ÂûãËææÂà∞ÂÖ∂ÁõÆÊ†áÊàñËÄÖËææÂà∞ÂÖ∂‰ª§ÁâåÈôêÂà∂‰∏∫Ê≠¢„ÄÇÂ¶ÇÊûúÊ®°ÂûãËøûÁª≠‰∏âÊ¨°ÈáçÂ§çÁõ∏ÂêåÁöÑËæìÂá∫ÔºåÊú¨ÊñáËÆ§‰∏∫ÂÆÉÊòØ‰∏Ä‰∏™ÈáçÂ§çÊÄßÁöÑÂ§±Ë¥•„ÄÇÂ¶ÇÊûúÊ®°ÂûãÁöÑËæìÂá∫Ê†ºÂºèÈîôËØØÔºåÊú¨Êñá‰ΩøÁî® BLEU ÊåáÊ†áÂ∞ÜÂÖ∂‰∏éÊâÄÊúâÂèØËÉΩÁöÑÊìç‰ΩúÈÄâÊã©ËøõË°åÊØîËæÉÔºåÂπ∂ÈÄâÊã©ÊúÄÊé•ËøëÁöÑÂåπÈÖç‰Ωú‰∏∫ËØ•Ê≠•È™§ÁöÑÊ®°ÂûãÊìç‰Ωú„ÄÇ</li><li><strong>CoT</strong> <strong>Rationales</strong> ÊÄùÁª¥Èìæ(CoT)ÊñπÊ≥ïÈÄöËøá‰∏ÄÊ≠•‰∏ÄÊ≠•ÁöÑÊé®ÁêÜËøáÁ®ãÊòæËëóÊèêÈ´ò‰∫Ü LLM ÁöÑÊé®ÁêÜËÉΩÂäõ(Wei et al„ÄÇ Ôºå2022b)„ÄÇÂõ†Ê≠§ÔºåÊú¨Êñá‰ΩøÁî® ReAct (Yao et al„ÄÇ Ôºå2023)‰Ωú‰∏∫Êé®ÁêÜÊ°ÜÊû∂ÔºåÂú®‰∫ßÁîüÊúÄÁªàÂä®‰Ωú‰πãÂâçËæìÂá∫ CoT Ëß£Èáä(Áß∞‰∏∫ÊÄùÊÉ≥)„ÄÇÂõ†Ê≠§ÔºåÂú®ÊâÄÊî∂ÈõÜÁöÑÁõ∏‰∫í‰ΩúÁî®ËΩ®Ëøπ‰∏≠ÁöÑÊØè‰∏Ä‰∏™Âä®‰ΩúÈÉΩ‰º¥ÈöèÁùÄ‰∏Ä‰∏™ËØ¶ÁªÜÁöÑËß£ÈáäË∑üË∏™Ôºå‰ΩøÊ®°ÂûãËÉΩÂ§üÂ≠¶‰π†ÂØºËá¥ËØ•Âä®‰ΩúÁöÑÊé®ÁêÜËøáÁ®ã„ÄÇÂØπ‰∫é‰ΩøÁî®Ê≤°ÊúâÊÄùÊÉ≥ÁöÑ‰ªªÂä°Ê¥æÁîüÁîüÊàêÁöÑËΩ®ËøπÔºåÊú¨Êñá‰ΩøÁî® GPT-4Êù•Ë°•ÂÖÖÂÆÉ‰ª¨Ôºå‰ª•‰æø‰∏é ReAct ÊèêÁ§∫‰øùÊåÅ‰∏ÄËá¥„ÄÇ</li></ol><h5 id="trajectory-filtering"><a href="#trajectory-filtering" class="headerlink" title="trajectory filtering"></a>trajectory filtering</h5><p>ÂåÖÂê´ÁúüÂÆûÂú∫ÊôØÁöÑ‰ª£ÁêÜ‰ªªÂä°Â∏¶Êù•‰∫ÜÂ∑®Â§ßÁöÑÊåëÊàò„ÄÇÂç≥‰ΩøÊòØ GPT-4‰πü‰∏çËÉΩÊª°Ë∂≥Ëøô‰∫õ‰ªªÂä°ÁöÑÊúüÊúõ„ÄÇ‰∏∫‰∫ÜÁ°Æ‰øùÊï∞ÊçÆË¥®ÈáèÔºåÊú¨Êñá‰∏•Ê†ºÁ≠õÈÄâ‰∫ÜÂÆÉÁöÑ‰∫§‰∫íËΩ®Ëøπ„ÄÇÂõûÊÉ≥‰∏Ä‰∏ãÔºåÊØè‰∏™‰∫§‰∫íËΩ®ËøπÈÉΩ‰ºöÂæóÂà∞‰∏Ä‰∏™Â•ñÂä± rÔºåËøô‰ΩøÂæóÊú¨ÊñáÂèØ‰ª•Ê†πÊçÆÂ•ñÂä±Ëá™Âä®ÈÄâÊã©È´òË¥®ÈáèÁöÑËΩ®Ëøπ„ÄÇÊú¨ÊñáÁ≠õÈÄâÊâÄÊúâ‰ªªÂä°ÁöÑËΩ®ËøπÔºåÈô§‰∫Ü Mind2WebÔºåÂü∫‰∫é r &#x3D; 1ÁöÑÊúÄÁªàÂ•ñÂä±ÔºåË°®ÊòéÂÆåÂÖ®Ê≠£Á°Æ„ÄÇÁÑ∂ËÄåÔºåÁî±‰∫é Mind2Web ‰ªªÂä°ÁöÑÂõ∞ÈöæÔºåÊú¨Êñá‰ΩøÁî® r ‚â•23ÁöÑÈòàÂÄºÊù•Á°Æ‰øùÊú¨ÊñáËé∑ÂæóË∂≥Â§üÊï∞ÈáèÁöÑËΩ®Ëøπ„ÄÇÂú®Ë°®2‰∏≠ÔºåÊú¨ÊñáÈÄöËøáÂú®7B Â∞∫Â∫¶‰∏ãÂØπËøáÊª§ÂíåÊú™ËøáÊª§ÁöÑËΩ®ËøπËøõË°åÂæÆË∞ÉÔºåËØÅÊòé‰∫ÜÊú¨ÊñáÁöÑËøáÊª§Á≠ñÁï•ÁöÑÊúâÊïàÊÄß„ÄÇ‰∏éÁªèËøáËøáÊª§ËΩ®ËøπËÆ≠ÁªÉÁöÑÊ®°ÂûãÁõ∏ÊØîÔºåÁªèËøáÊú™ÁªèËøáÊª§ËΩ®ËøπËÆ≠ÁªÉÁöÑÊ®°ÂûãÂú®Âõ∫ÂÆö‰ªªÂä°ÂíåÂõ∫ÂÆö‰ªªÂä°‰∏äÁöÑË°®Áé∞ÈÉΩË¶ÅÂ∑ÆÂæóÂ§ö„ÄÇËøôÂº∫Ë∞É‰∫Ü‰ª£ÁêÜ‰ªªÂä°ÁöÑÊï∞ÊçÆË¥®Èáè‰ºò‰∫éÊï∞ÊçÆÈáèÁöÑÈáçË¶ÅÊÄß„ÄÇ</p><h5 id="Instruct-tuning"><a href="#Instruct-tuning" class="headerlink" title="Instruct tuning"></a>Instruct tuning</h5><ol><li>‰ΩøÁî®‰∫Ü‰∏çÂêåÁî®Êà∑ÁöÑ‰∫§‰∫íËΩ®ËøπÔºàShareGPTÔºâ</li><li>Â∞Üagent‰ªªÂä°ÂíåÈÄöÁî®‰ªªÂä°Ê∑∑Âêà</li></ol><p><strong>ÂÆûÈ™å</strong></p><p>Áï•</p><p><strong>ÁªìËÆ∫</strong></p><p>sftÂèØ‰ª•ÁªôÊ®°ÂûãÊ≥®ÂÖ•agentËÉΩÂäõ</p>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;How-to-Make-a-Big-Language-Model-an-Agent&quot;&gt;&lt;a href=&quot;#How-to-Make-a-Big-Language-Model-an-Agent&quot; class=&quot;headerlink&quot; title=&quot;How to Mak</summary>
      
    
    
    
    
  </entry>
  
</feed>
